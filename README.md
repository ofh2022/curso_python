ğŸ“Š Projeto de Engenharia de Dados â€“ ETL com Airflow
ğŸ“Œ VisÃ£o Geral

Este projeto implementa um pipeline de ETL (Extract, Transform, Load) utilizando Apache Airflow para orquestraÃ§Ã£o, Python para processamento dos dados e Docker para padronizaÃ§Ã£o do ambiente.

Os dados sÃ£o extraÃ­dos de um banco PostgreSQL, transformados em Python e carregados em um SQL Server (Data Warehouse).

O ambiente foi desenvolvido utilizando WSL (Ubuntu) para desenvolvimento local e Docker Compose para execuÃ§Ã£o do Airflow.

ğŸ—ï¸ Arquitetura
PostgreSQL (Origem)
        |
        v
   Python ETL
 (TransformaÃ§Ãµes)
        |
        v
SQL Server (DW)
        |
        v
Apache Airflow (OrquestraÃ§Ã£o)

ğŸ“ Estrutura do Projeto
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_cliente_dag.py        # DAG do Airflow
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ cliente.py               # ETL de cliente
â”‚   â”œâ”€â”€ produto.py               # ETL de produto
â”‚   â””â”€â”€ vendas.py                # ETL de vendas
â”‚
â”œâ”€â”€ db_connection.py              # ConexÃµes com PostgreSQL e SQL Server
â”œâ”€â”€ docker-compose.yaml           # Infraestrutura do Airflow
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ .gitignore                    # Arquivos ignorados pelo Git
â”œâ”€â”€ .env                          # VariÃ¡veis de ambiente (NÃƒO versionado)
â””â”€â”€ README.md                     # DocumentaÃ§Ã£o do projeto

âš™ï¸ Tecnologias Utilizadas

Python 3.10+

Apache Airflow 3.1

Docker & Docker Compose

PostgreSQL

SQL Server

pyodbc

pandas

WSL (Ubuntu)

ğŸ” VariÃ¡veis de Ambiente

Crie um arquivo .env na raiz do projeto:

# PostgreSQL (Origem)
PG_HOST=localhost
PG_PORT=5432
PG_DB=database_origem
PG_USER=usuario
PG_PASS=senha

# SQL Server (Destino)
MSSQL_HOST=localhost
MSSQL_PORT=1433
MSSQL_DB=dw
MSSQL_USER=sa
MSSQL_PASS=senha


âš ï¸ Nunca versionar o arquivo .env

ğŸ³ Subindo o Airflow com Docker

Execute os comandos abaixo na pasta onde estÃ¡ o docker-compose.yaml:

docker-compose down
docker-compose up -d


Acesse a interface do Airflow em:

http://localhost:8080


Credenciais padrÃ£o:

UsuÃ¡rio: airflow

Senha: airflow

ğŸ“¦ Volume de CÃ³digo ETL no Airflow

O cÃ³digo ETL local Ã© montado no container via volume:

/home/orlando/curso_python  â†’  /opt/airflow/projetos


Isso permite que as DAGs importem diretamente os scripts Python sem copiÃ¡-los para a pasta dags.

â±ï¸ Exemplo de DAG (Cliente)
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import sys

sys.path.append("/opt/airflow/projetos")

from etl.cliente import etl_cliente

with DAG(
    dag_id="etl_cliente_postgres_to_sqlserver",
    start_date=datetime(2025, 1, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:

    PythonOperator(
        task_id="run_etl_cliente",
        python_callable=etl_cliente
    )

ğŸ”„ EstratÃ©gia de Carga

Staging / DimensÃµes simples

Carga FULL

TRUNCATE + INSERT

Fatos / HistÃ³rico (futuro)

Incremental

MERGE ou controle por data (watermark)

As regras de carga ficam no ETL, nunca na DAG.

ğŸ“ˆ Boas PrÃ¡ticas Aplicadas

SeparaÃ§Ã£o entre orquestraÃ§Ã£o (Airflow) e lÃ³gica de dados (ETL)

Uso de volumes Docker

Uso de PythonOperator

Ambiente isolado com .env

CÃ³digo versionado sem artefatos locais
